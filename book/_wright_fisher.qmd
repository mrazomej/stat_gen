### Beanbag genetics and the Wright-Fisher model

As it has hopefully become clear so far in the book, we are not afraid to make
gross oversimplifications about the process we are trying to model. After all,
these simplifications and idealizations allow us to make analytical and
computational progress, distilling the relevant features of complex phenomena.
Thus, unapologetically, we will continue returning to "beanbag genetics" as a
way to study evolution. Arguably, the most important beanbag genetics model is
the so-called Wright-Fisher model, independently created by two field
pioneers---Sewall Wright and Ronald Fisher. Ironically, the model carries both
names since Fisher and Wright disagreed and bitterly fought about almost every
topic in population genetics throughout their prolific careers.

The Wright-Fisher model assumes that a population of diploid organisms has a
fixed population size and non-overlapping generations. In other words, if at
generation $t$, there are $N$ organisms (therefore $2N$ alleles due to
diploidy), generation $t + 1$ will also have $N$ organisms. Still, none of the
ones from the previous generation will make it to the next one. Furthermore,
although sexual reproduction involves two different sexes---also known as mating
types---the model takes the beanbag genetics to the extreme. As schematized in
@fig-beanbag_genetics, new organisms are created by blindly grabbing one allele
from the bag, writing down its identity, and then putting it back in the bag to
repeat the process for the second allele. In other words, we sample both alleles
with replacement. This oversimplification allows us to write a simple
mathematical description of how the probability of having a particular allele
frequency in the population changes over time.

![Beanbag genetics schematic](./figures/placeholder.jpg){#fig-beanbag_genetics}

Thinking about the Buri experiment from the previous section, let us assume that
$N = 16$ individuals conform each generation. To stick to our previous notation,
we will define $A$ as the red-eye allele and $a$ as the white-eye allele.
Mathematically speaking, the Wright-Fisher model is an example of a so-called
discrete-time discrete-states Markov process. This means that the probability of
transitioning from one state to another––from one allele frequency in generation
$t-1$ to another in generation $t$––depends on the current state and not on the
entire history. In other words, to compute how likely a population is to jump
from having $Y_{t-1}$ copies of allele $A$ at generation $Y_{t}$ at generation
$t$, we do not need information from the population state in previous
generations. Notice that we define the allele copy number using an uppercase 
letter. Throughout the book, *random variables* will be defined using uppercase
letters. See [`cite prbability appendix`] for our notation conventions.

 Our objective is to build the transition probability $P(X_t=j \mid X_{t-1}=i)$
 of transitioning from having $i$ copies of, say, allele $A$ in generation $t-1$
 to $j$ copies in generation $t$. At generation $t$, the probability of drawing
 a copy of allele $A$, $p_A(t)$ is given by
$$
p_A(t) = \frac{i}{2N},
$${#eq-prob_A_wf}
where, as before, $i$ is the number of copies of allele $A$ in generation $t-1$.
Likewise, the probability of drawing a copy of allele $a$ at generation $t$ is
$$
p_a(t) = 1 - p_A(t) = 1 - \frac{i}{2N}.
$${#eq-prob_a_wf}
Given that each time we draw an allele, we replace it in the beanbag, each draw
is independent of the others. Thus, the probability of drawing, say, $j$ copies
of allele $A$ in consecutive trials is given as the multiplication of
@eq-prob_A_wf by itself $j$ times. Finally, to construct our transition
probability, we must take into account that drawing $j$ copies of allele $A$ and
$2N - j$ copies of allele $a$ can happen in many different ways (see [Math
behind the model @sec-math_behind_binomial_coefficient]), resulting in
$$
P(X_t=j \mid X_{t-1}=i) = 
\overbrace{\frac{2N!}{j! \left(2N - j\right)!}}
^{\text{order doesn't matter}}
\underbrace{\left( \frac{i}{2N}\right)^j}_
{\text{draw $j$ copies of $A$}}
\overbrace{\left( 1 - \frac{i}{2N} \right)^{2N - j}}^
{\text{draw $j - 2N$ copies of $a$}}.
$${#eq-pij_wf}

#### Properties of the Wright-Fisher model

With @eq-pij_wf in hand, let's now compute some of the important properties
of the Wright-Fisher model. First, let's compute the expected value for the
allele copy number $\langle X_t \rangle$. Since the information we have about
$X_t$ is conditioned on the value of $X_{t-1}$, we cannot directly compute
$\rangle X_t \langle$. $X_t \mid X_{t-1}$ is a random variable as any other. The
only difference is that we take the value of $X_t$ as random given that we
pretend to know the value of $X_{t-1}$. Therefore we can compute its expected
values as 
$$ 
\left\langle X_t \mid X_{t-1} \right\rangle_{X_t} = 
\sum_{j=0}^{2N} j \; P\left(X_t = j \mid X_{t-1}\right).
$${#eq-E_Xt_Xt-1_wf}
Notice that we use a subscript on the left-hand side to make it clear that the
expected value is taken over all possible values of $X_t$. One important feature
to highlight about @eq-E_Xt_Xt-1_wf is that $\left\langle X_t \mid X_{t-1}
\right\rangle_{X_t}$ is a random variable since $X_{t-1}$ needs not to take a
fixed value. [see `cite prob appendix` for conditional expectation]. Thus, to
compute the expected value we want, we must take a second expected value,
$$
\left\langle X_t \right\rangle = \left\langle \left\langle X_t \mid X_{t-1}
\right\rangle_{X_t} \right\rangle_{X_{t-1}}.
$${#eq-E_Xt_wf}
This is an instance of Adam's law of total expectation [see `cite prob
appendix`]. To compute @eq-E_Xt_wf, we first substitute @eq-E_Xt_Xt-1_wf,
obtaining
$$
\left\langle X_t \right\rangle = \left\langle
\sum_{j=0}^{2N} j \; P\left(X_t = j \mid X_{t-1}\right)
\right\rangle_{X_{t-1}}.
$${#eq-E_Xt_1_wf}
Next, we use the definition of the expected value to write
$$
\left\langle X_t \right\rangle = \sum_{i=0}^{2N} \left[ 
\sum_{j=0}^{2N} j \; P\left(X_t = j \mid X_{t-1} = i \right)
\right] P\left(X_{t-1} = i \right).
$${#eq-E_Xt_2_wf}
Rearranging the sums in @eq-E_Xt_2_wf results in
$$
\left\langle X_t \right\rangle = \sum_{i=0}^{2N} P\left( X_{t-1} = i\right)
\left[
\sum_{j=0}^{2N} j \; P\left(X_t = j \mid X_{t-1} = i \right)
\right].
$${#eq-E_Xt_3_wf}
Notice that the term inside the square brackets in @eq-E_Xt_3_wf is the expected
value of $X_t$ given a fixed value of $X_{t-1}$. From @eq-pij_wf, we know that
this conditional probability is binomial, making computing this expected value
straightforward. This allows us to write
$$
\left\langle X_t \right\rangle = \sum_{i=0}^{2N} P\left( X_{t-1} = i\right)
\left[
2N \left( \frac{i}{2N}\right)
\right].
$${#eq-E_Xt_4_wf}
Simplifying terms, we find that
$$
\left\langle X_t \right\rangle = \sum_{i=0}^{2N} i \; P\left( X_{t-1} = i\right)
= \left\langle X_{t-1} \right\rangle,
$${#eq-E_Xt_5_wf}
where for the second equality, we used the definition of the expected value of
$X_{t-1}$. Notice the curious result in @eq-E_Xt_5_wf. This is an iterative
relationship that must apply to all generations. Therefore, @eq-E_Xt_5_wf 
implies that
$$
\left\langle X_t \right\rangle =
\left\langle X_{t-1} \right\rangle =
\cdots
\left\langle X_{1} \right\rangle =
X_o.
$${#eq-E_Xt_iteration}
In our current setup, it does not make sense to take an expected value For the
initial condition $X_o$, since we assume this is not a random variable but a
constant––just like every vial in the Buri experiment started with the same
allele copy number. Therefore, our result leading to @eq-E_Xt_iteration implies
that the expected copy allele copy number at any generation is the same as the
initial condition. Although this result makes intuitive sense mathematically––on
average, an ensemble of populations will maintain the same allele copy number as
their initial condition––the intuition we are trying to capture has to do more
with populations deviating from this initial condition. For this, we should
compute the variance since we expect this quantity to increase over generations.

Just as we did for the mean, to compute the variance $\text{Var}(X_t)$, we must
take into account the dependence of $X_t$ on $X_{t-1}$. For this, we can use
Eve's law of total variance [see `cite prob appendix`] which states that
$$
\text{Var}(X_t) = \left\langle 
    \text{Var}_{X_t} \left(X_t \mid X_{t-1} \right) 
\right\rangle_{X_{t-1}} +
\text{Var}_{X_{t-1}} \left( 
    \left\langle X_t \mid X_{t-1} \right\rangle_{X_t}
\right),
$${#eq-VarXt_eves_wf}
where again the subindexes indicate over which random variable to compute the
statistic when not obvious. Given the binomial nature of @eq-pij_wf, we know the
functional form of both terms on the right-hand side of @eq-VarXt_eves_wf.
Substituting the mean and the variance of the binomial distribution results in
$$
\text{Var}(X_t) = \left\langle 
    2N \left(\frac{X_{t-1}}{2N} \right)\left( 1 - \frac{X_{t-1}}{2N} \right)
\right\rangle +
\text{Var} \left(
    \left\langle 2N \left(\frac{X_{t-1}}{2N}\right) \right\rangle
\right).
$${#eq-VarXt_1_wf}
Simplifying terms in @eq-VarXt_1_wf results in
$$
\text{Var}(X_t) - \left\langle 
    X_{t-1} - \frac{X_{t-1}^2}{2N}
\right\rangle +
\text{Var} \left( X_{t-1} \right)
$${#eq-VarXt_2_wf}
Using the linearity of expected values, we can split the first term on the 
right-hand side of @eq-VarXt_2_wf, and use our result from @eq-E_Xt_iteration to
write
$$
\text{Var}(X_t) = X_o - \frac{\left\langle X_{t-1}^2 \right\rangle}{2N} +
\text{Var}(X_{t-1}).
$${#eq-VarXt_3_wf}
To simplify @eq-VarXt_3_wf further, we use the fact that $\text{Var}(X) =
\langle X^2 \rangle - \langle X \rangle^2$ to substitute the second term on the
right-hand side, obtaining
$$
\text{Var}(X_t) = X_o -
\frac{1}{2N} \left( 
    \text{Var}(X_{t-1}) + \left\langle X_{t-1} \right\rangle^2
\right) +
\text{Var}(X_{t-1}).
$${#eq-VarXt_4_wf}
Grouping terms in @eq-VarXt_4_wf and using our result for the expected value
results in
$$
\text{Var}(X_t) = \text{Var}(X_{t-1}) \left(1 - \frac{1}{2N} \right) +
X_o \left( 1 - \frac{X_o}{2N} \right).
$${#eq-VarXt_5_wf}
@eq-VarXt_5_wf is already a recursive relationship that connects the variance in
generation $t$ to the variance in $t-1$. But the second added term on the
right-hand side makes this recursion very cumbersome to handle. To simplify the
relationship, we want the left- and the right-hand side to look more symmetric.
To achieve this symmetry, we must factor a $(1 - 1/2N)$ from both terms on the
right-hand side. This can be done by adding a factor of $2N X_o \left(1 -
\frac{X_o}{2N}\right)$ to both sides of @eq-VarXt_5_wf, obtaining
$$
\text{Var}(X_t) - 2N X_o \left(1 - \frac{1}{2N} \right) =
\left(1 - \frac{1}{2N} \right) \left[
    \text{Var}(X_{t-1}) - 2N X_o \left(1 - \frac{X_o}{2N} \right)
\right].
$${#eq-VarXt_6_wf}
Notice that this addition makes the recursive relationship symmetric. For
example, to relate the variance of $X_t$ with that of $X_{t-2}$ only one extra
multiplicative factor of $\left(1 - \frac{1}{2N} \right)$ needs to be added.
Thus, the general form of this recursive relationship takes the form
$$
\text{Var}(X_t) - 2N X_o \left(1 - \frac{1}{2N} \right) =
\left(1 - \frac{1}{2N} \right)^{r} \left[
    \text{Var}(X_{t-r}) - 2N X_o \left(1 - \frac{X_o}{2N} \right)
\right].
$${#eq-VarXt_7_wf}
We can take this relationship back to generation 0 and use the fact that since
the initial condition is fixed, it follows that $\text{Var}(X_o) = 0$. This
then results in
$$
\text{Var}(X_t) - 2N X_o \left(1 - \frac{1}{2N} \right) =
\left(1 - \frac{1}{2N} \right)^{t} \left[
    - 2N X_o \left(1 - \frac{X_o}{2N} \right)
\right].
$${#eq-VarXt_8_wf}
Solving for $\text{Var}(X_t)$ gives us our final result
$$
\text{Var}(X_t) = 2N X_o \left( 1 - \frac{X_o}{2N} \right)
\left[ 1 - \left( 1 - \frac{1}{2N} \right)^t \right].
$${#eq-VarXt_wf}
As a sanity check, we know that for $t = 1$ @eq-VarXt_wf must be equal to the
variance of the binomial distribution with $p = \frac{X_o}{2N}$. We leave it to
the reader to check that @eq-VarXt_wf result satisfies this result. More
interestingly, let us take the limit when $t \rightarrow \infty$.
$$
\lim_{t \rightarrow \infty} \text{Var}(X_t) = 
2N X_o \left( 1 - \frac{X_o}{2N} \right) =
(2N)^2 \left( \frac{X_o}{2N} \right) \left( 1 - \frac{X_o}{2N} \right) =
2N \;\text{Var}(X_1).
$${#eq-VarXt_infty_wf}
This means that in the long run, the variance in the allele copy number will be
2N times larger than the variance in the first generation.

- Plot dynamics (time evolution of distribution and stochastic simulations).
- Compare theory vs. data.
___
{{< include __math_behind_binomial_coefficient.qmd >}}
---